{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"654c794449bd42378ad0ff1ba20a3842":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_77ac0e5761554e789f001c1045129ea8","IPY_MODEL_3db63fd099ba4a2a8d3ffcaa147b6011","IPY_MODEL_5a5479ef19af4814b4a227ea44ffd90a"],"layout":"IPY_MODEL_60f4949511b44ae499946129cee86819"}},"77ac0e5761554e789f001c1045129ea8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_69079a20dea64e91885053bf2cc54a4c","placeholder":"‚Äã","style":"IPY_MODEL_5ca7b048742c49e986dd8e30727367a7","value":"model.safetensors:‚Äá100%"}},"3db63fd099ba4a2a8d3ffcaa147b6011":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8f26197afbe3469cb2d27ea32f7257ee","max":31471874,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d026f74db1264804b09f4b93da7bf720","value":31471874}},"5a5479ef19af4814b4a227ea44ffd90a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e52204a5e84f419994c7017fc2b3bbf8","placeholder":"‚Äã","style":"IPY_MODEL_0b074c585dd5460f892711429f1d78aa","value":"‚Äá31.5M/31.5M‚Äá[00:00&lt;00:00,‚Äá56.8MB/s]"}},"60f4949511b44ae499946129cee86819":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"69079a20dea64e91885053bf2cc54a4c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5ca7b048742c49e986dd8e30727367a7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8f26197afbe3469cb2d27ea32f7257ee":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d026f74db1264804b09f4b93da7bf720":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e52204a5e84f419994c7017fc2b3bbf8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0b074c585dd5460f892711429f1d78aa":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10783793,"sourceType":"datasetVersion","datasetId":6691299}],"dockerImageVersionId":30887,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Setup","metadata":{"id":"-dOaYbAh8flk"}},{"cell_type":"markdown","source":"# ‚òÄÔ∏è Solar Panel & Boiler Detection in Madagascar - Satellite & Drone Imagery Challenge\n\n## üìú Competition Overview\n\nAccess to reliable and sustainable energy is a critical issue in Madagascar and across Africa, where many communities either lack electricity or rely on costly, environmentally harmful energy sources. Solar technology offers a promising alternative ‚Äî but identifying and tracking the adoption of solar panels and boilers over large, remote areas remains a major challenge.\n\nThe goal of this competition is to develop a **machine learning model** capable of **detecting and counting** solar panels and solar boilers in satellite and drone imagery of Madagascar.\n\nA robust detection system has important real-world applications, including:\n- Supporting governments and NGOs with energy planning and policy\n- Monitoring the impact of renewable energy programs\n- Optimizing resource allocation for electrification efforts","metadata":{}},{"cell_type":"code","source":"!nvidia-smi","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2025-02-21T20:32:20.651526Z","iopub.execute_input":"2025-02-21T20:32:20.651853Z","iopub.status.idle":"2025-02-21T20:32:20.879920Z","shell.execute_reply.started":"2025-02-21T20:32:20.651816Z","shell.execute_reply":"2025-02-21T20:32:20.878713Z"},"id":"anJiNQwP6jq5","outputId":"7eed6c1a-fb25-47f9-afd6-f70a886dc94b","trusted":true},"outputs":[{"name":"stdout","text":"Fri Feb 21 20:32:20 2025       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n| N/A   50C    P8             10W /   70W |       1MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n|   1  Tesla T4                       Off |   00000000:00:05.0 Off |                    0 |\n| N/A   42C    P8             10W /   70W |       1MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n                                                                                         \n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n|  No running processes found                                                             |\n+-----------------------------------------------------------------------------------------+\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"## üì¶ Library Imports\n\nWe begin by importing all necessary libraries for the project, including:\n\n- **PyTorch** for deep learning model development\n- **Albumentations** for powerful image augmentations\n- **TIMM** for easy access to pretrained image models\n- **Pandas** and **NumPy** for data handling\n- **OpenCV** and **PIL** for image processing\n- **Scikit-learn** for cross-validation strategies\n- **TQDM** for progress visualization during training\n\nThese tools form the foundation of the modeling pipeline for solar panel and boiler detection.\n","metadata":{}},{"cell_type":"code","source":"import os\nimport timm\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nfrom torch.utils.data import Dataset, DataLoader\nimport pandas as pd\nimport cv2\nfrom sklearn.model_selection import StratifiedKFold\nimport numpy as np\nfrom tqdm import tqdm\nimport random\nfrom torchvision import transforms\nfrom PIL import Image, ImageEnhance","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2025-02-21T20:32:20.881538Z","iopub.execute_input":"2025-02-21T20:32:20.881820Z","iopub.status.idle":"2025-02-21T20:32:35.473099Z","shell.execute_reply.started":"2025-02-21T20:32:20.881796Z","shell.execute_reply":"2025-02-21T20:32:35.472136Z"},"id":"Y5T_1zXS9HRb","outputId":"a8747c79-80d9-4263-fcd2-a410ab24388e","trusted":true,"jupyter":{"outputs_hidden":true},"collapsed":true},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/albumentations/__init__.py:24: UserWarning: A new version of Albumentations is available: 2.0.4 (you have 1.4.20). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n  check_for_updates()\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"# Imports","metadata":{"id":"Bi_jqci98hjS"}},{"cell_type":"markdown","source":"## üìÇ Load Training Data\n\nWe load the provided training dataset, which contains:\n- Image identifiers\n- Corresponding labels (number of solar panels and boilers)\n\nThis structured data will be used to train and validate our model.\n","metadata":{}},{"cell_type":"code","source":"# Read in the training dataset\ntrain = pd.read_csv(\"/kaggle/input/lacuna-solar-survey-challenge/Train.csv\")\n\ntrain","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":112},"execution":{"iopub.status.busy":"2025-02-21T20:32:35.529838Z","iopub.execute_input":"2025-02-21T20:32:35.530202Z","iopub.status.idle":"2025-02-21T20:32:35.572163Z","shell.execute_reply.started":"2025-02-21T20:32:35.530168Z","shell.execute_reply":"2025-02-21T20:32:35.571004Z"},"id":"saWCS23p2EEK","outputId":"c160cee4-db1c-4950-c8ce-4b1a3fcaaf27","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## üõ†Ô∏è Preprocessing: Dataset Structuring\n\nWe perform preprocessing steps to better structure the dataset for model training:\n\n- Create mappers for `placement` and `img_origin` information linked to each image ID\n- Aggregate the `boil_nbr` and `pan_nbr` counts per unique image\n- Merge back additional metadata (`placement` and `img_origin`)\n- Generate the full file path for each image\n\nThis prepares a clean and enriched dataframe for further modeling.","metadata":{}},{"cell_type":"code","source":"# Create a placement mapper\nplacement_mapper = train[[\"ID\", \"placement\"]].drop_duplicates().set_index(\"ID\").to_dict()\n# Create a \"img_origin\" mapper\nimg_origin_mapper = train[[\"ID\", \"img_origin\"]].drop_duplicates().set_index(\"ID\").to_dict()\n\n# Group by \"ID\" and sum up boil_nb, pan_nbr\ntrain_df = train.groupby(\"ID\").sum().reset_index()[[\"ID\", \"boil_nbr\", \"pan_nbr\"]]\n\n# Map img_origin and placement\ntrain_df[\"img_origin\"] = train_df[\"ID\"].map(img_origin_mapper[\"img_origin\"])\ntrain_df[\"placement\"] = train_df[\"ID\"].map(placement_mapper[\"placement\"])\ntrain_df\n# Create path column\ntrain_df[\"path\"] = \"/kaggle/input/lacuna-solar-survey-challenge/images/\" + train_df[\"ID\"] + \".jpg\"\ntrain_df[train_df['boil_nbr']!=0] ","metadata":{"execution":{"iopub.status.busy":"2025-02-21T20:32:35.574205Z","iopub.execute_input":"2025-02-21T20:32:35.574606Z","iopub.status.idle":"2025-02-21T20:32:35.649467Z","shell.execute_reply.started":"2025-02-21T20:32:35.574579Z","shell.execute_reply":"2025-02-21T20:32:35.648348Z"},"id":"5DF3hPzM2wIG","trusted":true},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"                     ID  boil_nbr  pan_nbr img_origin  placement  \\\n19             ID0I1tHW         1        0          S  S-unknown   \n30            ID0POc2HN         1        0          S  S-unknown   \n32        ID0RNQgs4Y1a7         1        0          S  S-unknown   \n39    ID0YWEiDPjmAfA50E         1        0          S  S-unknown   \n44              ID0g0Yd         1        0          S  S-unknown   \n...                 ...       ...      ...        ...        ...   \n3287     IDzZw0Ot8e05Xu         1        0          S  S-unknown   \n3288   IDzaUuik1u5aOQt9         2        0          S  S-unknown   \n3295       IDzj0TllxhxN         1        0          S  S-unknown   \n3303           IDzpcgPa         1        0          S  S-unknown   \n3307         IDzt0z6lQC         1        0          S  S-unknown   \n\n                                                   path  \n19    /kaggle/input/lacuna-solar-survey-challenge/im...  \n30    /kaggle/input/lacuna-solar-survey-challenge/im...  \n32    /kaggle/input/lacuna-solar-survey-challenge/im...  \n39    /kaggle/input/lacuna-solar-survey-challenge/im...  \n44    /kaggle/input/lacuna-solar-survey-challenge/im...  \n...                                                 ...  \n3287  /kaggle/input/lacuna-solar-survey-challenge/im...  \n3288  /kaggle/input/lacuna-solar-survey-challenge/im...  \n3295  /kaggle/input/lacuna-solar-survey-challenge/im...  \n3303  /kaggle/input/lacuna-solar-survey-challenge/im...  \n3307  /kaggle/input/lacuna-solar-survey-challenge/im...  \n\n[489 rows x 6 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID</th>\n      <th>boil_nbr</th>\n      <th>pan_nbr</th>\n      <th>img_origin</th>\n      <th>placement</th>\n      <th>path</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>19</th>\n      <td>ID0I1tHW</td>\n      <td>1</td>\n      <td>0</td>\n      <td>S</td>\n      <td>S-unknown</td>\n      <td>/kaggle/input/lacuna-solar-survey-challenge/im...</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>ID0POc2HN</td>\n      <td>1</td>\n      <td>0</td>\n      <td>S</td>\n      <td>S-unknown</td>\n      <td>/kaggle/input/lacuna-solar-survey-challenge/im...</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td>ID0RNQgs4Y1a7</td>\n      <td>1</td>\n      <td>0</td>\n      <td>S</td>\n      <td>S-unknown</td>\n      <td>/kaggle/input/lacuna-solar-survey-challenge/im...</td>\n    </tr>\n    <tr>\n      <th>39</th>\n      <td>ID0YWEiDPjmAfA50E</td>\n      <td>1</td>\n      <td>0</td>\n      <td>S</td>\n      <td>S-unknown</td>\n      <td>/kaggle/input/lacuna-solar-survey-challenge/im...</td>\n    </tr>\n    <tr>\n      <th>44</th>\n      <td>ID0g0Yd</td>\n      <td>1</td>\n      <td>0</td>\n      <td>S</td>\n      <td>S-unknown</td>\n      <td>/kaggle/input/lacuna-solar-survey-challenge/im...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>3287</th>\n      <td>IDzZw0Ot8e05Xu</td>\n      <td>1</td>\n      <td>0</td>\n      <td>S</td>\n      <td>S-unknown</td>\n      <td>/kaggle/input/lacuna-solar-survey-challenge/im...</td>\n    </tr>\n    <tr>\n      <th>3288</th>\n      <td>IDzaUuik1u5aOQt9</td>\n      <td>2</td>\n      <td>0</td>\n      <td>S</td>\n      <td>S-unknown</td>\n      <td>/kaggle/input/lacuna-solar-survey-challenge/im...</td>\n    </tr>\n    <tr>\n      <th>3295</th>\n      <td>IDzj0TllxhxN</td>\n      <td>1</td>\n      <td>0</td>\n      <td>S</td>\n      <td>S-unknown</td>\n      <td>/kaggle/input/lacuna-solar-survey-challenge/im...</td>\n    </tr>\n    <tr>\n      <th>3303</th>\n      <td>IDzpcgPa</td>\n      <td>1</td>\n      <td>0</td>\n      <td>S</td>\n      <td>S-unknown</td>\n      <td>/kaggle/input/lacuna-solar-survey-challenge/im...</td>\n    </tr>\n    <tr>\n      <th>3307</th>\n      <td>IDzt0z6lQC</td>\n      <td>1</td>\n      <td>0</td>\n      <td>S</td>\n      <td>S-unknown</td>\n      <td>/kaggle/input/lacuna-solar-survey-challenge/im...</td>\n    </tr>\n  </tbody>\n</table>\n<p>489 rows √ó 6 columns</p>\n</div>"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"train_df.head(2)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":112},"execution":{"iopub.status.busy":"2025-02-21T20:32:35.651223Z","iopub.execute_input":"2025-02-21T20:32:35.651598Z","iopub.status.idle":"2025-02-21T20:32:35.662522Z","shell.execute_reply.started":"2025-02-21T20:32:35.651572Z","shell.execute_reply":"2025-02-21T20:32:35.661353Z"},"id":"ULcbkzcQ-T86","outputId":"59ca52bc-f36e-41ec-9590-a913ea89fd88","trusted":true},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"           ID  boil_nbr  pan_nbr img_origin placement  \\\n0     ID00rw8         0        2          D      roof   \n1  ID014O6EC7         0        1          D      roof   \n\n                                                path  \n0  /kaggle/input/lacuna-solar-survey-challenge/im...  \n1  /kaggle/input/lacuna-solar-survey-challenge/im...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID</th>\n      <th>boil_nbr</th>\n      <th>pan_nbr</th>\n      <th>img_origin</th>\n      <th>placement</th>\n      <th>path</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ID00rw8</td>\n      <td>0</td>\n      <td>2</td>\n      <td>D</td>\n      <td>roof</td>\n      <td>/kaggle/input/lacuna-solar-survey-challenge/im...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ID014O6EC7</td>\n      <td>0</td>\n      <td>1</td>\n      <td>D</td>\n      <td>roof</td>\n      <td>/kaggle/input/lacuna-solar-survey-challenge/im...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":6},{"cell_type":"markdown","source":"## üß© Cross-Validation Strategy: Stratified K-Fold\n\nTo ensure balanced distribution of multi-label targets across folds, we implement **Stratified K-Fold** cross-validation:\n\n- Create a `stratify_label` by summing `boil_nbr` and `pan_nbr` for each sample\n- Use **7 folds** with shuffling and a fixed random seed for reproducibility\n- Assign a `fold` index to each sample for organized training and validation splits\n\nThis method helps maintain class balance during training and evaluation.","metadata":{}},{"cell_type":"code","source":"# Stratified KFold based on multi-label targets\ntrain_df[\"stratify_label\"] = train_df[[\"boil_nbr\", \"pan_nbr\"]].sum(axis=1)\nskf = StratifiedKFold(n_splits=7, shuffle=True, random_state=42)\ntrain_df[\"fold\"] = -1\nfor fold, (_, valid_idx) in enumerate(skf.split(train_df, train_df[\"stratify_label\"])):\n    train_df.loc[valid_idx, \"fold\"] = fold","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2025-02-21T20:32:35.663680Z","iopub.execute_input":"2025-02-21T20:32:35.664003Z","iopub.status.idle":"2025-02-21T20:32:35.697968Z","shell.execute_reply.started":"2025-02-21T20:32:35.663972Z","shell.execute_reply":"2025-02-21T20:32:35.696826Z"},"id":"HKG3kLmc3U0j","outputId":"dec2c167-1473-4413-c7c6-68fc5b9ef7a8","trusted":true},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py:700: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=7.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"## üé® Data Augmentation Strategies\n\nTo improve the model‚Äôs robustness and prevent overfitting, we apply a combination of **Albumentations** and **PyTorch** data augmentations:\n\n### Custom Transformations:\n- **RandomSharpen**: Randomly increases image sharpness\n- **RandomBlur**: Randomly applies Gaussian blur\n\n### Albumentations Transforms (for training and testing):\n- Resizing, flipping, brightness/contrast adjustment\n- Shift, scale, rotate, elastic, and grid distortions\n- Normalization and conversion to tensor\n\n### PyTorch Transforms (for training and validation):\n- Resize to InceptionV3‚Äôs input size (299√ó299)\n- Random horizontal flip, sharpening, blurring\n- Normalization following ImageNet statistics\n\nThese augmentations simulate real-world conditions and improve generalization.","metadata":{}},{"cell_type":"code","source":"class RandomSharpen:\n    def __init__(self, probability=0.5):\n        self.probability = probability\n\n    def __call__(self, img):\n        if random.random() < self.probability:\n            enhancer = ImageEnhance.Sharpness(img)\n            img = enhancer.enhance(random.uniform(1.5, 2.0))  # Randomly increase sharpness\n        return img\n\nclass RandomBlur:\n    def __init__(self, probability=0.5):\n        self.probability = probability\n\n    def __call__(self, img):\n        if random.random() < self.probability:\n            img = np.array(img)\n            ksize = random.choice([3, 5])  # Randomly choose kernel size\n            img = cv2.GaussianBlur(img, (ksize, ksize), 0)\n            img = Image.fromarray(img)\n        return img\n\n# Define Albumentations transformations\nalbu_train_transforms = A.Compose([\n    A.Resize(256, 256),  # Resize the image to 256x256\n    A.HorizontalFlip(p=0.5),  # Apply horizontal flip with 50% probability\n    A.RandomBrightnessContrast(p=0.2),  # Randomly adjust brightness and contrast with 20% probability\n    A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.2, rotate_limit=15, p=0.5),  \n    # Randomly shift, scale, and rotate the image\n    A.GridDistortion(p=0.2),  # Apply grid distortion with 20% probability\n    A.ElasticTransform(p=0.2),  # Apply elastic transform with 20% probability\n    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # Normalize the image\n    ToTensorV2()  # Convert the image to a PyTorch tensor\n])\n\nalbu_test_transforms = A.Compose([\n    A.Resize(256, 256),  # Resize the image to 256x256\n    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # Normalize the image\n    ToTensorV2()  # Convert the image to a PyTorch tensor\n])\n\n# Define PyTorch transformations\ntorch_train_transforms = transforms.Compose([\n    transforms.Resize((299, 299)),  \n    transforms.RandomHorizontalFlip(),\n    RandomSharpen(probability=0.5),\n    RandomBlur(probability=0.5),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\n\ntorch_valid_transforms = transforms.Compose([\n    transforms.Resize((299, 299)),  # InceptionV3 expects 299x299 input size\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])","metadata":{"execution":{"iopub.status.busy":"2025-02-21T20:32:35.707407Z","iopub.execute_input":"2025-02-21T20:32:35.707767Z","iopub.status.idle":"2025-02-21T20:32:35.740150Z","shell.execute_reply.started":"2025-02-21T20:32:35.707731Z","shell.execute_reply":"2025-02-21T20:32:35.739108Z"},"id":"Wn5YdAa6DQru","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## üîÄ Combined Albumentations and PyTorch Transformations\n\nWe define a custom **CombinedTransform** class to seamlessly merge **Albumentations** and **PyTorch** transformations in a single pipeline:\n\n- First, apply powerful augmentations from Albumentations (e.g., resizing, distortions).\n- Then, convert the augmented image back to a **PIL** format.\n- Finally, apply additional PyTorch transformations (e.g., random blur, sharpening, normalization).\n\n### Purpose:\nThis approach leverages the strengths of both libraries, ensuring rich, diverse augmentations while maintaining compatibility with PyTorch DataLoaders.\n","metadata":{}},{"cell_type":"code","source":"# Custom transformation class to combine Albumentations and PyTorch transformations\nclass CombinedTransform:\n    def __init__(self, albu_transform, torch_transform):\n        self.albu_transform = albu_transform\n        self.torch_transform = torch_transform\n\n    def __call__(self, image):\n        # Apply Albumentations transformations\n        image = np.array(image)\n        augmented = self.albu_transform(image=image)\n        image = augmented['image']\n        \n        # Convert back to PIL image for PyTorch transformations\n        image = transforms.ToPILImage()(image)\n        \n        # Apply PyTorch transformations\n        image = self.torch_transform(image)\n        \n        return image\n\n# Combine transformations\ntrain_transforms = CombinedTransform(albu_train_transforms, torch_train_transforms)\nvalid_transforms = CombinedTransform(albu_test_transforms, torch_valid_transforms)","metadata":{"execution":{"iopub.status.busy":"2025-02-21T20:32:35.802030Z","iopub.execute_input":"2025-02-21T20:32:35.802358Z","iopub.status.idle":"2025-02-21T20:32:35.829750Z","shell.execute_reply.started":"2025-02-21T20:32:35.802320Z","shell.execute_reply":"2025-02-21T20:32:35.828756Z"},"id":"urT-xV7X_tKB","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## üèóÔ∏è Dataset, Early Stopping, and Model Architecture\n\nThis section builds the essential backbone for model training:\n\n### üì¶ Custom Dataset Class: `SolarPanelDataset`\n- Loads images based on paths in the dataframe.\n- Applies the combined Albumentations and PyTorch transformations.\n- Returns either:\n  - Image and target (boiler and panel counts) for training.\n  - Image only for inference.\n\n### ‚è≥ Early Stopping Strategy\n- A custom **EarlyStopping** class monitors validation loss.\n- Stops training early if no improvement is seen after a defined patience.\n- Saves the best model checkpoint automatically.\n\n### üß† Model Architecture: `InceptionRegressor`\n- Built upon a **pretrained InceptionV3** model using **timm**.\n- Modifies the final fully connected layer to output two values:\n  - **Boiler count** and **Solar panel count**.\n\nThis design ensures an efficient and stable training process focused on both predictive performance and overfitting prevention.","metadata":{}},{"cell_type":"code","source":"# Custom Dataset\nclass SolarPanelDataset(Dataset):\n    def __init__(self, dataframe, transform=None, to_train=True):\n        self.dataframe = dataframe\n        self.transform = transform\n        self.to_train = to_train\n\n    def __len__(self):\n        return len(self.dataframe)\n\n    def __getitem__(self, idx):\n        row = self.dataframe.iloc[idx]\n        image = Image.open(row[\"path\"]).convert(\"RGB\")\n        \n        if self.transform:\n            image = self.transform(image)\n\n        if self.to_train:\n            target = torch.tensor([row[\"boil_nbr\"], row[\"pan_nbr\"]], dtype=torch.float32)\n            return image, target\n        else:\n            return image\n\n\n\nclass EarlyStopping:\n    def __init__(self, patience=5, verbose=False):\n        self.patience = patience\n        self.verbose = verbose\n        self.counter = 0\n        self.best_score = None\n        self.early_stop = False\n\n    def __call__(self, val_loss, model, path):\n        score = -val_loss\n        if self.best_score is None:\n            self.best_score = score\n            self.save_checkpoint(val_loss, model, path)\n        elif score < self.best_score:\n            self.counter += 1\n            if self.counter >= self.patience:\n                self.early_stop = True\n        else:\n            self.best_score = score\n            self.save_checkpoint(val_loss, model, path)\n            self.counter = 0\n\n    def save_checkpoint(self, val_loss, model, path):\n        if self.verbose:\n            print(f'Validation loss decreased ({self.best_score:.6f} --> {val_loss:.6f}).  Saving model ...')\n        torch.save(model.state_dict(), path)\n\nimport timm\nimport torch.optim as optim\n\nclass InceptionRegressor(nn.Module):\n    def __init__(self):\n        super(InceptionRegressor, self).__init__()\n        self.model = timm.create_model(\"inception_v3\", pretrained=True)\n        # Replace the last fully connected layer\n        self.model.fc = nn.Linear(self.model.fc.in_features, 2)\n\n    def forward(self, x):\n        return self.model(x)","metadata":{"execution":{"iopub.status.busy":"2025-02-21T20:32:35.802030Z","iopub.execute_input":"2025-02-21T20:32:35.802358Z","iopub.status.idle":"2025-02-21T20:32:35.829750Z","shell.execute_reply.started":"2025-02-21T20:32:35.802320Z","shell.execute_reply":"2025-02-21T20:32:35.828756Z"},"id":"urT-xV7X_tKB","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## ‚öôÔ∏è Model Training Setup and Data Preparation\n\nThis section sets up the environment for training the solar panel and boiler detection model using an InceptionV3 backbone.\n\n### üõ† Model and Training Configuration\n- **Model**: `InceptionRegressor` based on a pretrained `inception_v3` model from `timm`.\n- **Loss Function**: L1 Loss (`nn.L1Loss`) for robust regression performance.\n- **Optimizer**: Adam optimizer with an initial learning rate of `1e-4`.\n- **Learning Rate Scheduler**: Reduces the learning rate by a factor of 0.1 if the validation loss plateaus for 5 epochs.\n- **Early Stopping**: Stops training if validation loss does not improve for 10 consecutive epochs.\n- **Device**: Automatically selects GPU if available, otherwise CPU.\n- **Logging**: TensorBoard `SummaryWriter` is initialized for monitoring training progress.\n\n### üì¶ Dataloader Preparation\n- **Stratified K-Fold** is used for creating training and validation splits based on the sum of boiler and panel counts.\n- **Fold Selection**: Fold 0 is used for validation; the remaining folds are used for training.\n- **Datasets**: \n  - `SolarPanelDataset` instances apply a combination of Albumentations and TorchVision transformations.\n- **DataLoaders**: \n  - **Training**: Batches of 32 images with shuffling enabled.\n  - **Validation**: Batches of 32 images without shuffling.\n  - **Parallelism**: Utilizes all available CPU cores for efficient data loading.","metadata":{}},{"cell_type":"code","source":"from torch.utils.tensorboard import SummaryWriter\n# Training Setup\nmodel = InceptionRegressor().cuda()\ncriterion = nn.L1Loss()  # MAE Loss\noptimizer = optim.Adam(model.parameters(), lr=1e-4)\nscheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=True)\nbest_model_path = \"best_model.pth\"\nnum_epochs = 50\nbest_loss = float(\"inf\")\nearly_stopping = EarlyStopping(patience=10, verbose=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\nwriter = SummaryWriter()\n\n# Prepare Dataloaders\nfold = 0  # Change fold index as needed\ntrain_data = train_df[train_df[\"fold\"] != fold].reset_index(drop=True)\nvalid_data = train_df[train_df[\"fold\"] == fold].reset_index(drop=True)\n\ndataset_train = SolarPanelDataset(train_data, transform=train_transforms)\ndataset_valid = SolarPanelDataset(valid_data, transform=valid_transforms)\n\ntrain_loader = DataLoader(dataset_train, batch_size=32, shuffle=True, num_workers=os.cpu_count())\nvalid_loader = DataLoader(dataset_valid, batch_size=32, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2025-02-21T20:35:21.758017Z","iopub.execute_input":"2025-02-21T20:35:21.758718Z","iopub.status.idle":"2025-02-21T20:35:21.769400Z","shell.execute_reply.started":"2025-02-21T20:35:21.758681Z","shell.execute_reply":"2025-02-21T20:35:21.768229Z"},"id":"5zg6m_dc9Va7","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import cv2\nimage = cv2.imread(\"/kaggle/input/lacuna-solar-survey-challenge/images/ID00rw8.jpg\")\nif image is not None:\n    print(\"Image loaded successfully:\", image.shape)\nelse:\n    print(\"Failed to load image.\")","metadata":{"execution":{"iopub.status.busy":"2025-02-21T20:35:22.678951Z","iopub.execute_input":"2025-02-21T20:35:22.679335Z","iopub.status.idle":"2025-02-21T20:35:22.925181Z","shell.execute_reply.started":"2025-02-21T20:35:22.679304Z","shell.execute_reply":"2025-02-21T20:35:22.924362Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Image loaded successfully: (3956, 5280, 3)\n","output_type":"stream"}],"execution_count":17},{"cell_type":"markdown","source":"## üöÄ Model Training Loop\n\nThe model is trained over a maximum of 50 epochs with early stopping and dynamic learning rate adjustment to optimize performance.\n\n### üîÑ Training and Validation Steps\n- **Training Phase**:\n  - Model set to training mode (`model.train()`).\n  - For each batch:\n    - Inputs and targets are loaded onto the device.\n    - Forward pass is performed.\n    - L1 loss is computed.\n    - Gradients are backpropagated, and the optimizer updates the model parameters.\n- **Validation Phase**:\n  - Model set to evaluation mode (`model.eval()`).\n  - No gradient computations (`torch.no_grad()`).\n  - Loss is computed for validation data.\n\n### üìà Monitoring and Optimization\n- **Loss Tracking**:\n  - Both training and validation losses are averaged and logged each epoch.\n- **Early Stopping**:\n  - Stops training if no improvement is observed for 10 consecutive validation evaluations.\n- **Learning Rate Scheduling**:\n  - Reduces learning rate when the validation loss plateaus to fine-tune learning.\n- **TensorBoard Logging**:\n  - Loss curves for both training and validation phases are recorded using TensorBoard's `SummaryWriter`.\n\n### üìù Notes\n- The best model is saved automatically during training whenever a new lowest validation loss is achieved.\n- If early stopping criteria are met, the training process halts early to prevent overfitting.","metadata":{}},{"cell_type":"code","source":"num_epochs = 50\nbest_loss = float(\"inf\")\nearly_stopping = EarlyStopping(patience=10, verbose=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\n\n# TensorBoard writer\nwriter = SummaryWriter()\n\n\n# Training Loop\nfor epoch in range(num_epochs):\n    # Training Phase\n    model.train()\n    epoch_loss = 0.0\n    for images, targets in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Training\"):\n        images, targets = images.to(device), targets.to(device)\n        optimizer.zero_grad()\n        outputs = model(images)\n        loss = criterion(outputs, targets)\n        loss.backward()\n        optimizer.step()\n        epoch_loss += loss.item()\n\n    # Validation Phase\n    model.eval()\n    val_loss = 0.0\n    with torch.no_grad():\n        for images, targets in tqdm(valid_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Validation\"):\n            images, targets = images.to(device), targets.to(device)\n            outputs = model(images)\n            loss = criterion(outputs, targets)\n            val_loss += loss.item()\n\n    # Average Loss\n    train_loss = epoch_loss / len(train_loader)\n    val_loss /= len(valid_loader)\n    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n\n    # Save Best Model\n    early_stopping(val_loss, model, best_model_path)\n    if early_stopping.early_stop:\n        print(\"Early stopping\")\n        break\n\n    # Adjust learning rate\n    scheduler.step(val_loss)\n\n    # Log to TensorBoard\n    writer.add_scalar('Loss/train', train_loss, epoch)\n    writer.add_scalar('Loss/val', val_loss, epoch)\n\n# Close the TensorBoard writer\nwriter.close()","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2025-02-21T20:37:39.770008Z","iopub.execute_input":"2025-02-21T20:37:39.770408Z","execution_failed":"2025-02-22T13:48:59.782Z"},"id":"D5gV2mYT_zFy","outputId":"0fb50c5a-93c1-46ee-b1fe-ed04556c821d","trusted":true},"outputs":[{"name":"stderr","text":"Epoch 1/50 - Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 89/89 [04:32<00:00,  3.06s/it]\nEpoch 1/50 - Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 15/15 [02:10<00:00,  8.68s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/50, Train Loss: 2.5297, Val Loss: 2.5917\nValidation loss decreased (-2.591667 --> 2.591667).  Saving model ...\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/50 - Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 89/89 [04:31<00:00,  3.05s/it]\nEpoch 2/50 - Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 15/15 [01:44<00:00,  6.95s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2/50, Train Loss: 2.4085, Val Loss: 2.5452\nValidation loss decreased (-2.545186 --> 2.545186).  Saving model ...\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/50 - Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 89/89 [04:21<00:00,  2.93s/it]\nEpoch 3/50 - Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 15/15 [01:39<00:00,  6.66s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3/50, Train Loss: 2.3204, Val Loss: 2.3424\nValidation loss decreased (-2.342386 --> 2.342386).  Saving model ...\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/50 - Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 89/89 [04:21<00:00,  2.94s/it]\nEpoch 4/50 - Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 15/15 [01:42<00:00,  6.84s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4/50, Train Loss: 2.2414, Val Loss: 2.3115\nValidation loss decreased (-2.311466 --> 2.311466).  Saving model ...\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/50 - Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 89/89 [04:25<00:00,  2.99s/it]\nEpoch 5/50 - Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 15/15 [01:44<00:00,  6.93s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5/50, Train Loss: 2.2329, Val Loss: 2.3413\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6/50 - Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 89/89 [04:27<00:00,  3.00s/it]\nEpoch 6/50 - Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 15/15 [01:45<00:00,  7.00s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6/50, Train Loss: 2.1061, Val Loss: 2.3249\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7/50 - Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 89/89 [04:26<00:00,  2.99s/it]\nEpoch 7/50 - Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 15/15 [01:43<00:00,  6.92s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7/50, Train Loss: 2.0905, Val Loss: 2.0902\nValidation loss decreased (-2.090153 --> 2.090153).  Saving model ...\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8/50 - Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 89/89 [04:22<00:00,  2.95s/it]\nEpoch 8/50 - Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 15/15 [01:45<00:00,  7.02s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8/50, Train Loss: 2.0729, Val Loss: 2.1595\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9/50 - Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 89/89 [04:27<00:00,  3.01s/it]\nEpoch 9/50 - Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 15/15 [01:46<00:00,  7.08s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9/50, Train Loss: 2.0843, Val Loss: 2.3030\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10/50 - Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 89/89 [04:27<00:00,  3.00s/it]\nEpoch 10/50 - Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 15/15 [01:45<00:00,  7.02s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10/50, Train Loss: 1.9804, Val Loss: 2.3726\n","output_type":"stream"},{"name":"stderr","text":"Epoch 11/50 - Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 89/89 [04:27<00:00,  3.01s/it]\nEpoch 11/50 - Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 15/15 [01:46<00:00,  7.11s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 11/50, Train Loss: 1.9625, Val Loss: 2.1586\n","output_type":"stream"},{"name":"stderr","text":"Epoch 12/50 - Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 89/89 [04:26<00:00,  2.99s/it]\nEpoch 12/50 - Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 15/15 [01:44<00:00,  6.97s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 12/50, Train Loss: 1.9380, Val Loss: 2.0857\nValidation loss decreased (-2.085653 --> 2.085653).  Saving model ...\n","output_type":"stream"},{"name":"stderr","text":"Epoch 13/50 - Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 89/89 [04:26<00:00,  3.00s/it]\nEpoch 13/50 - Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 15/15 [01:45<00:00,  7.03s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 13/50, Train Loss: 1.9990, Val Loss: 2.1706\n","output_type":"stream"},{"name":"stderr","text":"Epoch 14/50 - Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 89/89 [04:29<00:00,  3.02s/it]\nEpoch 14/50 - Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 15/15 [01:44<00:00,  6.98s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 14/50, Train Loss: 1.8853, Val Loss: 2.2651\n","output_type":"stream"},{"name":"stderr","text":"Epoch 15/50 - Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 89/89 [04:21<00:00,  2.94s/it]\nEpoch 15/50 - Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 15/15 [01:43<00:00,  6.88s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 15/50, Train Loss: 1.8964, Val Loss: 2.1222\n","output_type":"stream"},{"name":"stderr","text":"Epoch 16/50 - Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 89/89 [04:21<00:00,  2.94s/it]\nEpoch 16/50 - Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 15/15 [01:42<00:00,  6.82s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 16/50, Train Loss: 1.7932, Val Loss: 2.0859\n","output_type":"stream"},{"name":"stderr","text":"Epoch 17/50 - Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 89/89 [04:22<00:00,  2.95s/it]\nEpoch 17/50 - Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 15/15 [01:45<00:00,  7.06s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 17/50, Train Loss: 1.7698, Val Loss: 2.2128\n","output_type":"stream"},{"name":"stderr","text":"Epoch 18/50 - Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 89/89 [04:26<00:00,  2.99s/it]\nEpoch 18/50 - Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 15/15 [01:45<00:00,  7.03s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 18/50, Train Loss: 1.7957, Val Loss: 2.2559\n","output_type":"stream"},{"name":"stderr","text":"Epoch 19/50 - Training:  10%|‚ñà         | 9/89 [00:37<05:53,  4.42s/it]","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":"## üìä Model Evaluation\n\nAfter training, the best-performing model (based on validation loss) is loaded and evaluated on the validation dataset.\n\n### üîç Steps Performed\n- **Model Loading**:\n  - The model's parameters are restored from the saved checkpoint (`best_model.pth`).\n- **Prediction**:\n  - The model is set to evaluation mode.\n  - Forward passes are performed on the validation set without updating gradients.\n  - Predictions and true labels are collected for evaluation.\n- **Evaluation Metric**:\n  - The performance is assessed using **Mean Absolute Error (MAE)**, providing a straightforward interpretation of the model's average prediction error.\n\n### üìà Output\n- Displays the final **Validation MAE**, indicating the average number of panels/boilers the model is off by.\n\n\n","metadata":{}},{"cell_type":"code","source":"# Load Best Model\nmodel.load_state_dict(torch.load(best_model_path))\nmodel.eval()\n\n# Predict on Validation Set\npreds = []\ntrue_vals = []\nwith torch.no_grad():\n    for images, targets in tqdm(valid_loader, desc=\"Predicting on Validation Set\"):\n        images = images.cuda()\n        outputs = model(images).cpu().numpy()\n        preds.append(outputs)\n        true_vals.append(targets.numpy())\npreds = np.concatenate(preds, axis=0)\ntrue_vals = np.concatenate(true_vals, axis=0)\n\nfrom sklearn.metrics import mean_absolute_error\n\n# Evaluate using MAE\nmae = mean_absolute_error(true_vals, preds)\nprint(f\"Validation MAE: {mae:.4f}\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2025-02-21T20:32:38.002442Z","iopub.status.idle":"2025-02-21T20:32:38.002743Z","shell.execute_reply":"2025-02-21T20:32:38.002598Z"},"id":"QKQh0lfnPwqq","outputId":"0e820462-63b0-406d-bf14-ca0fb8050633","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## üöÄ Generating Test Predictions and Submission\n\nAfter validating the model, predictions are generated for the unseen **test set**.\n\n### üõ†Ô∏è Steps Performed\n- **Prepare Test Data**:\n  - Test images are read and preprocessed using the validation transforms.\n- **Model Inference**:\n  - The trained model predicts the number of boilers and panels for each test image.\n- **Submission File Creation**:\n  - Predictions are formatted according to the competition submission rules:\n    - For each image ID, two rows are created: one for boilers (`_boil`) and one for panels (`_pan`).\n  - Predicted values are clipped between **0** and **1000** to ensure reasonable outputs.\n- **Saving Submission**:\n  - A `Submission.csv` file is generated and saved for final submission.\n\n### üìÑ Output\n- `Submission.csv` containing model predictions ready for evaluation.","metadata":{}},{"cell_type":"code","source":"# Predict on Test Set\ntest_df = pd.read_csv(\"/kaggle/input/lacuna-solar-survey-challenge/Test.csv\")\n\ntest_df[\"path\"] = \"/kaggle/input/lacuna-solar-survey-challenge/images/\" + test_df[\"ID\"] + \".jpg\"\n\ndataset_test = SolarPanelDataset(test_df, transform=valid_transforms, to_train=False)\ntest_loader = DataLoader(dataset_test, batch_size=32, shuffle=False)\n\ntest_preds = []\nwith torch.no_grad():\n    for images in tqdm(test_loader, desc=\"Predicting on Test Set\"):\n        images = images.cuda()\n        outputs = model(images).cpu().numpy()\n        test_preds.append(outputs)\ntest_preds = np.concatenate(test_preds, axis=0)\n\n# Create Sample Submission\nsubmission = pd.DataFrame()\nsubmission[\"ID\"] = np.repeat(test_df[\"ID\"].values, 2)\nsubmission[\"ID\"] = submission[\"ID\"] + np.tile([\"_boil\", \"_pan\"], len(test_df))\nsubmission[\"Target\"] = test_preds.flatten().clip(0,1000)\n\n# Save Submission\nsubmission.to_csv(\"Submission.csv\", index=False)\nprint(\"Submission saved!\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2025-02-21T20:32:38.004698Z","iopub.status.idle":"2025-02-21T20:32:38.005053Z","shell.execute_reply":"2025-02-21T20:32:38.004895Z"},"id":"cepD5bBiP2Fz","outputId":"e833f539-2ec0-447f-e68d-1aea953c5e93","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}